{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "from colorama import Fore, Style\n",
    "from datetime import datetime\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator_factory import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.INFO)\n",
    "model_path = './model_files/gpt2-xl/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Model name './model_files/gpt2-xl/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming './model_files/gpt2-xl/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "INFO: Didn't find file ./model_files/gpt2-xl/added_tokens.json. We won't load it.\n",
      "INFO: Didn't find file ./model_files/gpt2-xl/special_tokens_map.json. We won't load it.\n",
      "INFO: Didn't find file ./model_files/gpt2-xl/tokenizer_config.json. We won't load it.\n",
      "INFO: loading file ./model_files/gpt2-xl/vocab.json\n",
      "INFO: loading file ./model_files/gpt2-xl/merges.txt\n",
      "INFO: loading file None\n",
      "INFO: loading file None\n",
      "INFO: loading file None\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path, output_loading_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('CUDA device is available')\n",
    "    model.to('cuda')\n",
    "else:\n",
    "    print('CUDA device missing')\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text_from_text(input_text, config, verbose=False):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    input_ids = torch.tensor(tokenizer.encode(input_text)).unsqueeze(0)\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids.to('cuda')\n",
    "        \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = generate(\n",
    "            model,\n",
    "            input_ids=torch.tensor(tokenizer.encode(text)).unsqueeze(0),\n",
    "            max_length=config['output_length'],\n",
    "            do_sample=True,\n",
    "            num_beams=config['num_beams'],\n",
    "            temperature=config['temperature'],\n",
    "            top_k=50,\n",
    "            top_p=1.0,\n",
    "            repetition_penalty=config['repetition_penalty'],\n",
    "            bos_token_id=0,\n",
    "            pad_token_id=0,\n",
    "            eos_token_ids=[0],\n",
    "            length_penalty=1.0,\n",
    "            num_return_sequences=config['num_return_sequences']\n",
    "        )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    if verbose:\n",
    "        logging.info(' Finished Text From Text Generation in {:4.2f} seconds.'.format(elapsed_time))\n",
    "    \n",
    "    if config_num_return_sequences == 1:\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    else:\n",
    "        pass\n",
    "#         for i in range(config_num_return_sequences):\n",
    "#             print('Generated {}: {}'.format(i, tokenizer.decode(outputs[0][i], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text_from_file(in_path, config, out_path=None, verbose=False):\n",
    "    original_file_articles = []\n",
    "    generated_texts = []\n",
    "    input_texts = []\n",
    "    \n",
    "    with open(in_path, 'r') as r_file:\n",
    "        for i, line in enumerate(r_file):\n",
    "            json_tmp = json.loads(line)\n",
    "            \n",
    "            # split the first part until \\n\\n, which is the title and only use the\n",
    "            # amount of characters given by input length\n",
    "            input_text = json_tmp['text'].split('\\n\\n', 1)[1][:config['input_length']]\n",
    "            # make sure that input text is word split (not character split) as this\n",
    "            # generates higher quality output\n",
    "            input_text = input_text[::-1].split(' ', 1)[1][::-1]\n",
    "            input_texts.append(input_text)\n",
    "            \n",
    "            generated_text = gen_text_from_text(input_text, config)\n",
    "            generated_texts.append(generated_text)\n",
    "            \n",
    "            # original input is temporarily saved to later be written into the finally formatted files\n",
    "            original_file_articles.append(json_tmp)\n",
    "\n",
    "    if not out_path:\n",
    "        return generated_texts\n",
    "    \n",
    "    with open(out_path, 'w') as w_file:\n",
    "        for i, article in enumerate(original_file_articles):\n",
    "            article.pop('url', None)\n",
    "            max_text_size = min(len(generated_texts[i]), len(article['text']))\n",
    "            \n",
    "            metadata = {\n",
    "                id: article['id'],\n",
    "                input: input_texts[i]\n",
    "            }    \n",
    "            ff_gpt2 = {\n",
    "                meta: metadata,\n",
    "                label: 1,  # machine,\n",
    "                title: article['title'],\n",
    "                text: generated_texts[i][:max_text_size]\n",
    "            }\n",
    "            ff_human = {\n",
    "                meta: metadata,\n",
    "                label: 0,  # human\n",
    "                title: article['title'],\n",
    "                text: article['text'][:max_text_size]\n",
    "            }\n",
    "            \n",
    "            w_file.write(json.dumps(ff_human) + '\\n')\n",
    "            w_file.write(json.dumps(ff_gpt2) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_for_folder(in_path, config, file_range=range(1), verbose=False):\n",
    "    all_files = []\n",
    "    # get all files (also in subdirs)\n",
    "    for root, directories, filenames in os.walk(in_path):\n",
    "        for filename in filenames:\n",
    "            if '.' in filename:\n",
    "                continue\n",
    "            else:\n",
    "                all_files.append(os.path.join(root, filename))\n",
    "\n",
    "    all_files = sorted(all_files)\n",
    "    total_files = len(all_files)\n",
    "    \n",
    "    info_string = '{:5}|{:50}|{:10}|{:20}|{:6.2f}'\n",
    "    if verbose:\n",
    "        logging.info('{:5}|{:50}|{:10}|{:20}|{:9}'.format('No.', 'Filename', 'Status', 'Time', 'Elapsed (seconds)'))\n",
    "\n",
    "    for i, file_path in enumerate(all_files):\n",
    "        from pathlib import Path\n",
    "        # get output path of file\n",
    "        split_path = file_path.split('/')\n",
    "        split_path[2] = 'output_after_gen'\n",
    "        out_path = '/'.join(split_path) + '.jsonl'\n",
    "        \n",
    "        \n",
    "        if i in file_range:\n",
    "            if verbose:\n",
    "                start_time = time.time()\n",
    "                current_time = datetime.now().strftime(\"%H:%M:%S\")  # get time in hours:minutes:seconds\n",
    "                logging.info(info_string.format(i, file_path, 'Started', current_time, 0.00))\n",
    "            Path(out_path[:-7]).mkdir(parents=True, exist_ok=True)\n",
    "            gen_text_from_file(file_path, config, out_path)\n",
    "            if verbose:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                current_time = datetime.now().strftime(\"%H:%M:%S\")  # get time in hours:minutes:seconds\n",
    "                logging.info(info_string.format(i, file_path, 'Finished', current_time, elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = './data/output_before_gen/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'input_length': 60,  # in characters\n",
    "    'output_length': 50,  # in tokens\n",
    "    'num_beams': 5,\n",
    "    'temperature': 1.0,\n",
    "    'repetition_penalty': 1.3,\n",
    "    'num_return_sequences': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:No.  |Filename                                          |Status    |Time                |Elapsed (seconds)\n",
      "INFO:    0|./data/output_before_gen/partition00/AA/wiki_00   |Started   |12:14:34            |  0.00\n",
      "INFO:    0|./data/output_before_gen/partition00/AA/wiki_00   |Finished  |12:14:34            |  0.06\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "generate_text_for_folder(input_folder, config, file_range=range(1), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12067\n",
      "./data/output_before_gen/partition00/AA/wiki_00\n",
      "./data/output_before_gen/partition00/AA/wiki_01\n",
      "./data/output_before_gen/partition00/AA/wiki_02\n",
      "./data/output_before_gen/partition00/AA/wiki_03\n",
      "./data/output_before_gen/partition00/AA/wiki_04\n"
     ]
    }
   ],
   "source": [
    "print(len(af))\n",
    "print(*af[:5], sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
